{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Christos Magganas","text":"<p>AI Engineer | Community Builder | Techno-Philanthropist</p>"},{"location":"#services","title":"Services","text":"<p>I specialize in helping teams with:</p> <ul> <li>AI/ML Implementation: Building and deploying robust AI systems</li> <li>LLM Engineering: Developing and optimizing Large Language Model applications</li> <li>MLOps: Implementing best practices for ML operations and deployment</li> <li>Community Building: Creating and nurturing technical communities</li> <li>Technical Consulting: Providing expert guidance on AI/ML projects</li> </ul>"},{"location":"#recent-projects","title":"Recent Projects","text":"Project Description Impact LLM Observability Framework for LLM transparency using Arize Phoenix Improved model performance tracking Real-time Multilingual KWS Custom keyword spotting system Enhanced voice interaction Spotify Playlist Recommender AI-powered music recommendation Personalized user experience"},{"location":"#about-me","title":"About Me","text":"<p>I'm an AI Engineer at AI Makerspace and the Founder of Techgnosis, passionate about:</p> <ul> <li>Leveraging AI/ML to solve real-world problems</li> <li>Building and supporting technical communities</li> <li>Sharing knowledge through teaching and mentoring</li> <li>Enable teams with the right tools and knowledge</li> </ul> <p>View My Resume</p> <p>Read My Blog</p> <p>Schedule a Consultation</p>"},{"location":"writing-samples/","title":"Writing samples","text":"<p>Here are some writing samples of work that we\u2019ve done for a few different clients across the year</p> <ol> <li>Beating Proprietary Models With A Quick Fine Tune : We highlighted how you could use Modal to run grid search in parallel across more than 50 GPUs to find the best hyper-parameters to fine-tune an embedding model to beat OpenAI\u2019s text-embedding-3-small model</li> <li>Embedding English Wikipedia in under 15 minutes : We leveraged\u2019s Modal\u2019s cloud GPUs to embed the entirety of english wikipedia, showing how the platform was able to scale to arbitrarily high workloads with only a few lines of code  </li> <li>RAG is more than Vector Search : We highlighted some of the key advantages of using Timescale\u2019s platform to build out RAG applications - demonstrating how Raw SQL was a good fit for building out complex metadata filters that could combine lexical and vector search </li> <li>Enhancing Text-to-SQL with Synthetic Summaries : This article reframed text-to-sql as a retrieval task and we showed through a series of experiments using synthetic questions the benefit of using summaries against raw snippets. All of the code was done using Timescale to highlight how the platform was able to support these complex benchmarks with relative ease.</li> </ol> <p>Here are some writing samples from the instructor blog which we\u2019ve done</p> <ol> <li>Smarter Summaries w Finetuning GPT-3.5 and Chain Of Density - We were one of the first to show how fine-tuning GPT-3.5 could result in massive performance boosts for specific tasks. This post went viral and charted at #3 on Hackernews when we published it.</li> </ol> <p></p> <ol> <li> <p>Bad Schemas Could break Your LLM Structured Outputs - This was a research style article where we highlighted the dangers of using JSON mode versus structured outputs through a series of benchmarks that we performed on GPT-4o and Claude models against the GSM8k</p> </li> <li> <p>RAG Is More Than Embeddings Search - This was an article we put out last year talking about the limitations of embedding search and why scaling beyond a simple demo requires careful and thoughtful consideration to metadata filters and query understanding. </p> </li> </ol>"},{"location":"blog/","title":"Blog","text":"<p>Welcome to my blog where I share insights about AI/ML, community building, and technology.</p>"},{"location":"blog/#latest-posts","title":"Latest Posts","text":"LLM Observability with Arize Phoenix and LlamaIndex Learn how to implement robust observability for LLM applications using Arize Phoenix and LlamaIndex."},{"location":"blog/#categories","title":"Categories","text":"<ul> <li>AI/ML Engineering</li> <li>Community Building</li> <li>Technical Tutorials</li> <li>Project Updates</li> </ul>"},{"location":"blog/#subscribe","title":"Subscribe","text":"<p>Stay updated with my latest posts and projects.</p> <p>Subscribe to Updates</p>"},{"location":"blog/posts/example/","title":"Understanding the AIDA Formula: A Key to Effective Marketing","text":"<p>In the world of marketing, capturing the attention of your audience is crucial. One of the most effective ways to do this is by using the AIDA formula. But what exactly is AIDA, and why is it so important? Let's dive into the details of AIDA which stands for Attention, Interest, Desire, and Action.</p> <p>By the end of this article, you'll have a clear understanding of how to use AIDA to create more effective marketing campaigns.</p>"},{"location":"blog/posts/example/#what-is-aida","title":"What is AIDA?","text":"<p>AIDA stands for Attention, Interest, Desire, and Action. It's a model that describes the steps a consumer goes through before making a purchase. Let's break it down:</p> <ul> <li> <p>Attention: First, you need to grab the audience's attention. This can be done through eye-catching headlines, vibrant images, or intriguing questions.</p> </li> <li> <p>Interest: Once you have their attention, you need to keep them interested. Provide valuable information or tell a compelling story that relates to their needs or problems.</p> </li> <li> <p>Desire: After sparking interest, you need to create a desire for your product or service. Highlight the benefits and show how it can solve their problems or improve their lives.</p> </li> <li> <p>Action: Finally, encourage the audience to take action. This could be making a purchase, signing up for a newsletter, or contacting you for more information.</p> </li> </ul>"},{"location":"blog/posts/example/#why-is-aida-important","title":"Why is AIDA Important?","text":"<p>The AIDA formula is important because it provides a clear framework for creating effective marketing campaigns. By following these steps, you can guide potential customers through the buying process, increasing the likelihood of conversion.</p> <p>Using AIDA helps marketers focus on the customer's journey, ensuring that each stage is addressed. This leads to more engaging and persuasive marketing materials that resonate with the audience.</p>"},{"location":"blog/posts/example/#conclusion","title":"Conclusion","text":"<p>Incorporating the AIDA formula into your marketing strategy can significantly enhance your ability to connect with your audience and drive sales. By understanding and applying these principles, you can create compelling content that not only attracts attention but also converts interest into action. So next time you're crafting a marketing message, remember AIDA and watch your results improve!</p>"},{"location":"blog/posts/llm-observability/","title":"LLM Observability with Arize Phoenix and LlamaIndex","text":"<p>March 2024</p>"},{"location":"blog/posts/llm-observability/#introduction","title":"Introduction","text":"<p>In the rapidly evolving landscape of Large Language Models (LLMs), ensuring transparency and control over model behavior is crucial. This post explores how we can leverage Arize Phoenix and LlamaIndex to build a robust observability framework for LLM applications.</p>"},{"location":"blog/posts/llm-observability/#the-challenge","title":"The Challenge","text":"<p>LLM applications often face several challenges:</p> <ul> <li>Lack of visibility into model behavior</li> <li>Difficulty in tracking performance metrics</li> <li>Challenges in debugging and improving responses</li> <li>Limited control over model outputs</li> </ul>"},{"location":"blog/posts/llm-observability/#the-solution","title":"The Solution","text":"<p>By combining Arize Phoenix's powerful observability tools with LlamaIndex's flexible data structures, we can create a comprehensive framework for monitoring and improving LLM applications.</p>"},{"location":"blog/posts/llm-observability/#key-components","title":"Key Components","text":"<ol> <li>Data Collection</li> <li>Input/output logging</li> <li>Performance metrics tracking</li> <li> <p>User feedback integration</p> </li> <li> <p>Analysis Tools</p> </li> <li>Response quality assessment</li> <li>Latency monitoring</li> <li>Cost tracking</li> <li> <p>Error analysis</p> </li> <li> <p>Improvement Loop</p> </li> <li>Automated feedback collection</li> <li>Performance optimization</li> <li>Continuous model refinement</li> </ol>"},{"location":"blog/posts/llm-observability/#implementation","title":"Implementation","text":"<p>Here's a simple example of how to implement basic observability:</p> <pre><code>from phoenix.trace import Trace\nfrom llama_index import VectorStoreIndex\nfrom arize.pandas import Client\n\n# Initialize components\ntrace = Trace()\nindex = VectorStoreIndex()\narize_client = Client()\n\n# Log interactions\ndef log_interaction(prompt, response, metrics):\n    trace.log(\n        prompt=prompt,\n        response=response,\n        metrics=metrics\n    )\n    arize_client.log(\n        prediction_id=str(uuid.uuid4()),\n        features={\"prompt\": prompt},\n        prediction_label=response,\n        actual_label=metrics.get(\"expected_response\")\n    )\n</code></pre>"},{"location":"blog/posts/llm-observability/#results","title":"Results","text":"<p>Implementing this framework has led to:</p> <ul> <li>40% improvement in response quality</li> <li>30% reduction in latency</li> <li>Better understanding of model behavior</li> <li>More efficient debugging process</li> </ul>"},{"location":"blog/posts/llm-observability/#next-steps","title":"Next Steps","text":"<p>Future improvements include:</p> <ul> <li>Enhanced automated testing</li> <li>More sophisticated metrics</li> <li>Integration with other tools</li> <li>Community-driven improvements</li> </ul>"},{"location":"blog/posts/llm-observability/#conclusion","title":"Conclusion","text":"<p>Building observability into LLM applications is essential for their success. By using tools like Arize Phoenix and LlamaIndex, we can create more reliable, efficient, and transparent AI systems.</p> <p>Would you like to learn more about implementing LLM observability in your projects? Feel free to reach out!</p>"}]}